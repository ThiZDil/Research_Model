{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 nltk pandas google-colab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hves4ANRpCOR",
        "outputId": "df0df536-ed11-478f-8a09-4ce4b6d23bb8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: google-colab in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.27.0)\n",
            "Requirement already satisfied: ipykernel==5.5.6 in /usr/local/lib/python3.10/dist-packages (from google-colab) (5.5.6)\n",
            "Requirement already satisfied: ipyparallel==8.8.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (8.8.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (7.34.0)\n",
            "Requirement already satisfied: notebook==6.5.5 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.5.5)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.10/dist-packages (from google-colab) (1.5.2)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.32.3)\n",
            "Requirement already satisfied: tornado==6.3.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.3.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (4.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (6.1.12)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (0.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=18 in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (24.0.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython==7.34.0->google-colab)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (4.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (3.1.4)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3->google-colab) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3->google-colab) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3->google-colab) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3->google-colab) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.34.0->google-colab) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.5->google-colab) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google-colab) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (3.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.34.0->google-colab) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-colab) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.27.0->google-colab) (0.6.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook==6.5.5->google-colab) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.5->google-colab) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook==6.5.5->google-colab) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.2.2)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2, jedi\n",
            "Successfully installed PyPDF2-3.0.1 jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Load labeled dataset\n",
        "labeled_data_path = '/content/drive/MyDrive/Research/My/PreprocessedData/labeled_ksa_dataset.csv'\n",
        "data = pd.read_csv(labeled_data_path)\n",
        "\n",
        "# Check the structure of the data\n",
        "print(data.head())\n",
        "\n",
        "# Preprocess the data\n",
        "X = data['Text']  # Features (resume text)\n",
        "y = data['Label']  # Labels (KSA categories)\n",
        "\n",
        "# Split the data into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the text data into numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train an SVM classifier\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHad5mFYlBiU",
        "outputId": "e1956c73-f175-4954-d6f4-af4e87f0e7f8"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   Resume ID                                               Text      Label\n",
            "0          6  70% Back-end (Java/Spring/SQL/NoSQL) 30% Front...     Skills\n",
            "1          6  Knowledge of Java, JavaScript, TypeScript, C#,...  Knowledge\n",
            "2          6  Participated in design and development of an e...  Abilities\n",
            "3          6  Responsible for achieving target KPI during cu...  Abilities\n",
            "4          7  2 years of experience in frontend development ...     Skills\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Abilities       0.57      0.57      0.57         7\n",
            "   Knowledge       1.00      0.43      0.60         7\n",
            "      Skills       0.72      0.93      0.81        14\n",
            "\n",
            "    accuracy                           0.71        28\n",
            "   macro avg       0.76      0.64      0.66        28\n",
            "weighted avg       0.75      0.71      0.70        28\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for SVM\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the tuned model\n",
        "y_pred = best_model.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HwlLfry4Hzq",
        "outputId": "64123040-b44c-4d09-ed46-9136bcb3a087"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Abilities       0.56      0.71      0.62         7\n",
            "   Knowledge       1.00      0.43      0.60         7\n",
            "      Skills       0.81      0.93      0.87        14\n",
            "\n",
            "    accuracy                           0.75        28\n",
            "   macro avg       0.79      0.69      0.70        28\n",
            "weighted avg       0.80      0.75      0.74        28\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "gKzr9e9jpAf0"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming you have the trained model and TF-IDF vectorizer\n",
        "# Save the model to a file\n",
        "with open('/content/drive/MyDrive/Research/My/Model/model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "# Save the TF-IDF vectorizer to a file\n",
        "with open('/content/drive/MyDrive/Research/My/Model/vectorizer.pkl', 'wb') as vec_file:\n",
        "    pickle.dump(vectorizer, vec_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "# Load the trained model and vectorizer\n",
        "with open('/content/drive/MyDrive/Research/My/Model/model.pkl', 'rb') as model_file:\n",
        "    model = pickle.load(model_file)\n",
        "\n",
        "with open('/content/drive/MyDrive/Research/My/Model/vectorizer.pkl', 'rb') as vec_file:\n",
        "    vectorizer = pickle.load(vec_file)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            text += reader.pages[page_num].extract_text()\n",
        "        return text\n",
        "\n",
        "# Function to split text into sentences/paragraphs\n",
        "def split_into_segments(text):\n",
        "    # Splitting based on full stops, new lines, etc.\n",
        "    segments = re.split(r'(?<=[.!?]) +|\\n', text)\n",
        "    return [segment.strip() for segment in segments if len(segment.strip()) > 0]\n",
        "\n",
        "# Path to the new resume (PDF) you want to test\n",
        "new_resume_path = '/content/drive/MyDrive/Research/My/Test/50.pdf'\n",
        "\n",
        "# Step 1: Convert the new resume to text\n",
        "new_resume_text = extract_text_from_pdf(new_resume_path)\n",
        "\n",
        "# Step 2: Split the text into segments (sentences/paragraphs)\n",
        "resume_segments = split_into_segments(new_resume_text)\n",
        "\n",
        "# Step 3: Preprocess each segment using the trained TF-IDF vectorizer\n",
        "resume_tfidf = vectorizer.transform(resume_segments)\n",
        "\n",
        "# Step 4: Predict the KSAs for each segment\n",
        "predicted_ksas = model.predict(resume_tfidf)\n",
        "\n",
        "# Step 5: Output the results\n",
        "for segment, ksa in zip(resume_segments, predicted_ksas):\n",
        "    print(f\"Text: {segment}\\nPredicted KSA: {ksa}\\n{'-'*50}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOA_qoHiqLa0",
        "outputId": "d96d2e8b-b79d-4ab5-b510-fd7e9ca37ce5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Full-time, on-site, remote or hybrid.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: DOB: 1984.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Aliyah: .\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Residence: Kiryat Ono, Montefiore, 13.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: My name is Maria, Im a new repatriate from Russia, made an Aliyah in April.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: I work as a\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: senior system\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: analyst, a senior business analyst, a team leader and a project manager.\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: Understanding the importance of language in my profession, Im actively studying Hebrew.\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: Now I hope to\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: find a position of junior/middle system and business analyst or QA tester that will give me\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: the opportunity to\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: apply my skills and develop as I learn Hebrew.\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: I really love my job so Im ready to work hard and learn fast to develop rapidly in analytics\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: and leadership in\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: new conditions.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: I started my career as SQL and PL/SQL developer.\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: Then I changed my profile to a system\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: and business\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: analyst, combining these roles and role of QA tester throughout my career.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Thanks to\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: communication skills,\n",
            "Predicted KSA: Knowledge\n",
            "--------------------------------------------------\n",
            "Text: high level of emotional intelligence and the ability to see the big picture, took the team\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: leader position.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Later,\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: became the leader of several teams, combining leadership with the role of an analyst.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Have\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: also been the\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: project manager of several projects.\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: Now I have extensive experience in system analytics, business analytics, QA and\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: management.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Many years of\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: experience working for the largest banks in Russia.\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: Research, definition, optimization and\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: automation of\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: business processes, development of management and analytical reporting, all types of\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: manual testing, team\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: management and project management.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Work experience with Jira, Confluence, Visio, TFS,\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: PowerPoint,\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Visual Studio Code, MS Project, Oracle SQL Navigator, MS SQL Management studio etc.\n",
            "Predicted KSA: Knowledge\n",
            "--------------------------------------------------\n",
            "Text: Using\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: waterfall,\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: scram, kanban.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Some of my projects:\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: Electronic document management for cash deposit and withdrawal (Alfa-Bank).\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: 90% of\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: transactions\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: go through ED M.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Complete automation of deal making processes for collection and self-collection (Alfa-\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Bank).\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Integration of electronic document storage system with the main bank's systems.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Standardization of the\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: integration process for quick connection of systems (Al fa-Bank).\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Field Staff Management System of the Technical Support Department of Sberbank (AT\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Consulting).\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: System for monitoring and analyzing the activities of internal structural divisions of\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Sberbank (AT\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: Consulting).\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Joint Stock Company ALFA-BANK\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: September 2019  Senior Team Manager\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: currently working\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Process management of four teams: work planning, communication, risk management, team\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: building, human research, requirement review, assessment review, mentoring.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: December 2017 Senior System Analyst, Project Lead\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: September\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Automation of the process of concluding a cash collection deal.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: DBMS MsSql.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: 2019\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: All kinds of requirements and documentation, analyst team leader, management of all\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: processes\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: development, testing and communications.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: AT Consulting\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: August 2012   Senior System Analyst\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: December 2017\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Field Staff Management System of the Technical Support Department of Sberbank.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: System for monitoring and analyzing the activities of internal structural divisions of\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Sberbank\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: DBMS Oracle.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: All kinds of requirements and documentation, Analyst team leader, Management of all\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: processes of\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: analytics, testing and communication with the customer.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: E-school\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: April 2011  Senior System Analyst, Project Lead\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: August 2012\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Software for education system.\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: DBMS MsSql.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Business analysis, System analysis, Project management, Database architecture\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: development,\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Technical documentation, User documentation, testing.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: PROMOS\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: February 2009  PL/SQL, SQL Developer\n",
            "Predicted KSA: Knowledge\n",
            "--------------------------------------------------\n",
            "Text: August 2012\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Building sector.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: DBMS Oracle.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Form builder, Report builder, Database architecture development, Interviewing customers,\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: instruction manuals for systems.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Beeline Russia October 2008  SQL Developer\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: January 2009\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Telecommunications.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Big data reports developer.\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: INPRIS\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: September 2005  PL/SQL Developer, SQL Developer\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: September 2008\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Software for publishing houses.\n",
            "Predicted KSA: Abilities\n",
            "--------------------------------------------------\n",
            "Text: DBMS Oracle.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Form builder, Report builder, Database architecture development, Interviewing customers,\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: instruction manuals for systems.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Languages:\n",
            "Predicted KSA: Knowledge\n",
            "--------------------------------------------------\n",
            "Text: English\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Professional working proficiency\n",
            "Predicted KSA: Knowledge\n",
            "--------------------------------------------------\n",
            "Text: Russian\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Native\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Hebrew\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Elementary proficiency\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: Education:\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n",
            "Text: 2001  2003 Moscow State University, Faculty of Mechanics and Mathematics.\n",
            "Predicted KSA: Skills\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Full Code to Fine-Tune BERT:\n",
        "**"
      ],
      "metadata": {
        "id": "PdHovvlP4ma9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Research/My/PreprocessedData/labeled_ksa_dataset.csv')\n",
        "\n",
        "# Encode the labels (Knowledge: 0, Skills: 1, Abilities: 2)\n",
        "label_encoder = LabelEncoder()\n",
        "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,  # You can adjust this based on your data\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "X_train_tokenized = tokenize_function(list(X_train))\n",
        "X_test_tokenized = tokenize_function(list(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP9bjt9uDi22",
        "outputId": "54125e8c-cfb5-457e-b316-604e48a10c70"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # 3 labels for KSA\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paw3MNW5Dn46",
        "outputId": "686dfc42-9bc8-45e2-c844-021033005696"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    evaluation_strategy=\"epoch\",     # evaluate each epoch\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    num_train_epochs=3,              # number of epochs\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "def convert_to_hf_dataset(tokenized_inputs, labels):\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "# Convert train and test sets to Hugging Face Dataset\n",
        "train_dataset = convert_to_hf_dataset(X_train_tokenized, list(y_train))\n",
        "test_dataset = convert_to_hf_dataset(X_test_tokenized, list(y_test))\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "fFxIxs3qDrXO",
        "outputId": "50835fc0-3397-4229-90ab-1190b336ae64"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 03:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.812401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.732744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717975</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=21, training_loss=0.7318045752389091, metrics={'train_runtime': 214.8303, 'train_samples_per_second': 1.55, 'train_steps_per_second': 0.098, 'total_flos': 6845060008080.0, 'train_loss': 0.7318045752389091, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTglkIbHKfDX",
        "outputId": "86e89283-cf51-4d64-9dd5-7405bd5d5f0a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the compute_metrics function for accuracy\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)  # Get the predicted class with the highest score\n",
        "    accuracy = accuracy_score(p.label_ids, preds)  # Calculate accuracy manually\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Update the Trainer with the compute_metrics function\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics  # Pass the manual accuracy function\n",
        ")\n",
        "\n",
        "# Train and evaluate the model\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Print loss and accuracy\n",
        "print(f\"Test Loss: {results['eval_loss']}\")\n",
        "print(f\"Test Accuracy: {results['eval_accuracy']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "8qUGw9oqI1Ue",
        "outputId": "ee015054-0bce-4982-ee1d-79faea7795d4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 04:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.777147</td>\n",
              "      <td>0.678571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.652160</td>\n",
              "      <td>0.785714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.622135</td>\n",
              "      <td>0.785714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6221352815628052\n",
            "Test Accuracy: 0.7857142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"/path_to_your_fine_tuned_model\"  # Path to your fine-tuned BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)  # Use PyPDF2 to read PDF\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            text += reader.pages[page_num].extract_text()\n",
        "        return text\n",
        "\n",
        "# Function to split text into sentences/paragraphs\n",
        "def split_into_segments(text):\n",
        "    # Splitting based on full stops, new lines, etc.\n",
        "    segments = re.split(r'(?<=[.!?]) +|\\n', text)\n",
        "    return [segment.strip() for segment in segments if len(segment.strip()) > 0]\n",
        "\n",
        "# Step 1: Extract the resume from the PDF\n",
        "new_resume_path = '/content/drive/MyDrive/Research/My/Test/50.pdf'\n",
        "new_resume_text = extract_text_from_pdf(new_resume_path)\n",
        "\n",
        "# Step 2: Split the text into segments (sentences or paragraphs)\n",
        "resume_segments = split_into_segments(new_resume_text)\n",
        "\n",
        "# Step 3: Preprocess each segment using the BERT tokenizer\n",
        "tokenized_segments = tokenizer(\n",
        "    resume_segments,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,  # You can adjust this based on your data\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Step 4: Predict the KSAs for each segment\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenized_segments)\n",
        "    predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "# Map the predicted labels back to KSA classes\n",
        "label_map = {0: \"Knowledge\", 1: \"Skills\", 2: \"Abilities\"}  # Ensure this matches your label mapping\n",
        "predicted_ksas = [label_map[pred.item()] for pred in predictions]\n",
        "\n",
        "# Step 5: Output the results\n",
        "for segment, ksa in zip(resume_segments, predicted_ksas):\n",
        "    print(f\"Text: {segment}\\nPredicted KSA: {ksa}\\n{'-'*50}\")\n"
      ],
      "metadata": {
        "id": "JX0tvwbBIoTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}